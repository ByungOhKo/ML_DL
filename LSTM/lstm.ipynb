{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batch(encoded, batch_size, sequence_length):\n",
    "    # start_idx ~ end_idx 는 batch를 완전하게 구성할 수 있는 범위로 골라진다.\n",
    "    start_idx = random.randint(0, len(encoded) - batch_size * sequence_length - 1)\n",
    "    end_idx = start_idx + batch_size * sequence_length + 1\n",
    "    text_tensor = encoded[start_idx:end_idx]\n",
    "    text_tensor = torch.tensor(text_tensor)\n",
    "    # Shape of input, target: (N=batch_size, L=sequence_length)\n",
    "    text_input = torch.zeros((batch_size, sequence_length))\n",
    "    text_target = torch.zeros((batch_size, sequence_length))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        text_input[i, :] = (text_tensor[i*sequence_length:(i+1)*sequence_length])\n",
    "        text_target[i, :] = (text_tensor[i*sequence_length+1:(i+1)*sequence_length+1])\n",
    "\n",
    "    return text_input.long(), text_target.long()\n",
    "\n",
    "def get_batch_SGD(encoded, batch_size, sequence_length, shuffle):\n",
    "    full_length = len(encoded)\n",
    "    batch_length = batch_size * sequence_length\n",
    "    batch_per_epoch = full_length // (batch_length)\n",
    "    \n",
    "    encoded = encoded[0:batch_length * batch_per_epoch + 1]\n",
    "    encoded = np.array(encoded)\n",
    "    \n",
    "    input = np.zeros((batch_per_epoch, batch_length))\n",
    "    target = np.zeros((batch_per_epoch, batch_length))\n",
    "    \n",
    "    for i in range(batch_per_epoch):\n",
    "        input[i, :] = encoded[i*batch_length:(i+1)*batch_length]\n",
    "        target[i, :] = encoded[i*batch_length+1:(i+1)*batch_length+1]\n",
    "        \n",
    "    input = input.reshape((batch_per_epoch, batch_size, sequence_length))\n",
    "    target = target.reshape((batch_per_epoch, batch_size, sequence_length))\n",
    "    \n",
    "    if shuffle==True:\n",
    "        randperm_mini_batch = np.random.permutation(batch_per_epoch)\n",
    "        input = input.take(randperm_mini_batch, axis=0)\n",
    "        target = target.take(randperm_mini_batch, axis=0)\n",
    "        \n",
    "    for idx in range(input.shape[0]):\n",
    "        yield input[idx], target[idx]\n",
    "    #return input, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embed = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # input x: (N, L)\n",
    "        out = self.embed(x)\n",
    "        # after embedding: (N, L, vocab_size)\n",
    "        out, (hidden, cell) = self.lstm(out, (hidden, cell))\n",
    "        # after LSTM: (N, L, output_size)\n",
    "        out = self.fc(out)\n",
    "        # after FC: (N, L, vocab_size)\n",
    "        return out, (hidden, cell)\n",
    "    \n",
    "    def init_state(self, batch_size):\n",
    "        hidden = torch.zeros((self.num_layers, batch_size, self.hidden_size), requires_grad = True).to(device)\n",
    "        cell = torch.zeros((self.num_layers, batch_size, self.hidden_size), requires_grad = True).to(device)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/shakespear.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "chars = tuple(set(text))\n",
    "vocab_size = len(chars)\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: idx for idx, ch in int2char.items()}\n",
    "encoded = [char2int[ch] for ch in text] \n",
    "#encoded = torch.tensor([char2int[ch] for ch in text]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50) (10, 50)\n"
     ]
    }
   ],
   "source": [
    "batches = get_batch_SGD(encoded, 10, 50, False)\n",
    "x, y = next(iter(batches))\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# LSTM Parameters\n",
    "hidden_dim = 512\n",
    "num_layers = 3\n",
    "dropout = 0.5\n",
    "batch_size = 2\n",
    "sequence_length = 50\n",
    "\n",
    "learning_rate = [1e-4]\n",
    "\n",
    "epochs = 50\n",
    "print_every = 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "# model = LSTM(vocab_size, hidden_dim, num_layers, dropout, vocab_size).to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "# model.train()\n",
    "\n",
    "# loss_list = []\n",
    "\n",
    "# for epoch in range(1, epochs + 1):\n",
    "#     input, target = get_random_batch(encoded, batch_size, sequence_length)\n",
    "#     hidden, cell = model.init_state(batch_size)\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "#     loss = 0\n",
    "    \n",
    "#     input = input.to(device)\n",
    "#     target = target.to(device)\n",
    "#     target = F.one_hot(target, vocab_size).float()\n",
    "    \n",
    "#     pred, (hidden, cell) = model(input, hidden, cell)\n",
    "#     loss = criterion(pred, target)\n",
    "\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "        \n",
    "#     if epoch % print_every == 0:\n",
    "#         loss_list.append(loss.item())\n",
    "#         print(f'Train Epoch: {epoch}/{epochs} \\tLoss: {loss.item():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [04:36<18:38, 27.96s/it]"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.auto import trange\n",
    "\n",
    "for lr in learning_rate:\n",
    "    print(f\"learning rate: {lr}\")\n",
    "    model = LSTM(vocab_size, hidden_dim, num_layers, dropout, vocab_size).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    \n",
    "    # epoch\n",
    "    for epoch in trange(1, epochs + 1):  \n",
    "        batches = get_batch_SGD(encoded, batch_size, sequence_length, shuffle=False)\n",
    "        with tqdm(batches, unit=\"batch\", leave=False) as tepoch:\n",
    "        # mini-batch\n",
    "            for input, target in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                \n",
    "                input_ = torch.from_numpy(input).long().to(device)\n",
    "                target_ = torch.from_numpy(target).long().to(device)\n",
    "                target_ = F.one_hot(target_, vocab_size).float()\n",
    "                hidden, cell = model.init_state(batch_size)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                pred, (hidden, cell)= model(input_, hidden, cell)\n",
    "                loss = criterion(pred, target_)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "                \n",
    "            loss_list.append(loss.item())\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()\n",
    "# for epoch in range(1, epochs + 1):  \n",
    "#     batches = get_batch_SGD(encoded, batch_size, sequence_length, shuffle=False)\n",
    "#     # mini-batch\n",
    "#     for i, (input, target) in enumerate(batches):\n",
    "#         optimizer.zero_grad()\n",
    "#         hidden, cell = model.init_state(batch_size)\n",
    "\n",
    "#         input_ = torch.from_numpy(input).long()\n",
    "#         input_ = input_.to(device)\n",
    "#         target_ = torch.from_numpy(target).long()\n",
    "#         target_ = target_.to(device)\n",
    "#         target_ = F.one_hot(target_, vocab_size).float()\n",
    "        \n",
    "#         pred, (hidden, cell)= model(input_, hidden, cell)\n",
    "#         loss = criterion(pred, target_)\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         if i % print_every == 0:\n",
    "#             loss_list.append(loss.item())\n",
    "#             print(f'Training... {i}/{epoch} \\tLoss: {loss.item():.4f}')    \n",
    "\n",
    "#     print(f'Epoch: {epoch}/{epochs} complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5250b11f50>]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(initial_str=\"D\", predict_len=10000, sequence_length=1):\n",
    "    model.eval()\n",
    "    hidden, cell = model.init_state(1)\n",
    "    \n",
    "    initial_tensor = torch.zeros((1,sequence_length)).long()\n",
    "    initial_tensor[0][0] = char2int[initial_str]\n",
    "\n",
    "    predicted = initial_str\n",
    "\n",
    "    last_char = initial_tensor\n",
    "\n",
    "    for p in range(predict_len):\n",
    "        output, (hidden, cell) = model(last_char.to(device), hidden, cell)\n",
    "        topi = torch.argmax(output, dim=2)\n",
    "        predicted_char = int2char[topi.item()]\n",
    "        predicted += predicted_char\n",
    "        \n",
    "        last_char = torch.zeros((1, sequence_length)).long()\n",
    "        last_char[0][0] = char2int[predicted_char]\n",
    "        \n",
    "    return predicted\n",
    "\n",
    "generated_text = generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dout;;\n",
      "Mive; but wowp'n;;;hip;? of they; about thee? Pillowaty; thou; welb; you??\n",
      "PPexzozed\n",
      "Miseres;;WWithsbuck; Pornous;aPny word; Poncous;\n",
      "Pexence; apjen; then; who; Potpor?; Potod; Prot'pappuremoxs;WWet,; Pomon? with hoppesomes; this; with up?xPPoxot,; to;; ats, Peppow, his jojjyly??\n",
      "PPHEMIM::Wth jother;?WVPonjouj??WPPofover;;\n",
      "Pomex;; I would? Powecot vuse; it; with;?\n",
      "Pyxyotujetrey; at thou?jjjucest, as Porvilmes;;WWetougegh'sweotly; with; then; I kook??\n",
      "PPMizsor:\n",
      "Petat;; them,'s mint?? but axpment;; Phince;\n",
      "Pof's; Portoa; put;oPexnot? woucconfess!WPorce;monce;;\n",
      "Poos; quict;; withen; axdexnomy; of theyere; seven\n",
      "Wouldxst tilk beipt?ppporous;oof; Pencom? to; Perpop;\n",
      "Futx vexyous; Pown;; Pentiple, Pundous, with?\n",
      "We???PPPomotor:\n",
      "Wommand;;Wat;; you? kow? Most thou; are; Porbe;\n",
      "toxpyocuppeloverejjevej;; then; was; took; Poxsoup;\n",
      "withxpexofer;; then'fortencise jutlove;\n",
      "Worj's valtanter cobjent; jicconvery; Piltiencce;;PPobby, Pome; away; you; good;;\n",
      "Morj; peoper; with;jjetgo?? good; Mompot;;joul; peapy; Gow;; Petcemport;;Wwhep;;Wat; you? joy?? My?Vower;\n",
      "Whose tizz; and wextojjjeven;; Poxcopjused;\n",
      "Wowelj queen; Pelking upobetunizer;; utcexnone! beibuse;;\n",
      "Hep; Gony; Pome; would; you; poull?? by heart perform'd,;cchybourse gaxeb'blejjy?; Phate; Portoset?\n",
      "ppexsomeble; wound; Putio?? Pubotuse! with?????\n",
      "Potost;\n",
      "Wett? Howen;; Pencoust jrezzly; Poolens, Parcious;jWuply?\n",
      "PPoRPPot:\n",
      "Why,? for; your? ow were? a juxjfent; Potnower;;\n",
      "PPillest; Porpose; Pory; my;Woth; thowese;; then;;\n",
      "Penbeyad;; for themyold cauzes? thou? to jecture; as;\n",
      "Pexten; botnowem;; Penicuse? bewoude;;;\n",
      "Mex; Good;; Petlop; whece;; atjot??cptyou; lowe;; and you; pawcell;\n",
      "Wowp;;y;; pewep;;jjotegit wowj; Phicosjy? Pance;\n",
      "Potker;;Ppt;;Pthep; and they will; jeccuxe?? Woy; Postop?\n",
      "Whece;jougjfence to furtiperby; Withex;WPoxking?\n",
      "Gox; Pictomy putsosej?jPPPixot:\n",
      "there?' op, quen? of they??\n",
      "PPPEMIMES:\n",
      "What that? my?VVatixxexged? Phich;;ppose;\n",
      "Pot; Phalowy;optjet peapt???\n",
      "JJEPthoMester; woman;; ponesby; my; with;; them;;PPresy; Potopy; hope; cembery; that; whose;jlet;;\n",
      "Which wather; thisjjft'st to fuztonyjejj??\n",
      "PPPozot:\n",
      "My port, your coptazes?-WWith???PPPiloto;;\n",
      "Pot Portop's; prepentes; they; are; toul;;bPy?PPPPEXIP:\n",
      "It coming?\n",
      "WPecough;; aple; to me;; by; you; powert;;\n",
      "Peopebout; bloust; whotesjpepor? when;; them; Polton;PPore;\n",
      "Pex;;; then; would; they nowle?d? with;??WPPloxesom;;Ont; thou ubboy? ot welp; Inmizes; to wither;apte;\n",
      "When; to;jjence; Pougizs; that too come?ngut;\n",
      "thex'd; asjjutcest asjjoth, my sword;;WWowkongut theu;; Puppyojoptpoperom;;\n",
      "Fech were; and jostopepor; I'll pexxommynubes;\n",
      "Which batinger;; it; notkPerjedy? to they; koonkepp?\n",
      "Goodout!\n",
      "Furmest;aandent; somethis topecule;\n",
      "For, that ot Prazins; that we'll before?\n",
      "\n",
      "MoRenZent;\n",
      "PirstoHot;jjet?o-thy juster? Panyoto;;\n",
      "conkmens! butcPonfers; by these; jozz; for though'dst and;\n",
      "popty, poresven come; for;Pworb;; thes;; my;GPot; tome;; thou; Ifppeare?d better;\n",
      "Forseote; to; Porkos?? wholyotopperoved?\n",
      "PPoz;;ot; go, Porpous; whece;Wout togetever?--ViMbture:\n",
      "Woum, thiselves?? and thes? of;eng;; Pongomep;;Wot;;\n",
      "Popres; Peraty; with; je;; it; between; 'power;;-Gitles; up;ow without? awout?? Prowex? Pure? Purve!!\n",
      "PoPjoser;\n",
      "Por then; alont? Pownow; when;; I to;??PPMMMore:\n",
      "Why; to;y? Potiple?? Praze?? Porous; Pores?; put;;wwelce; would jesture; Purse; the; come;\n",
      "But perpose;;; Ponce, will; gook?\n",
      "PPIrop:\n",
      "Weth?? Vero???PPPEcozseM:\n",
      "Perenot; Vesbanjjject, as I wollfwy;\n",
      "Vouls twouch?monfer;; on thest tomes; Ifful;;\n",
      "When, you? not?PPProvized;; theny;oand;; Pongom; Morjedy Poonwe;\n",
      "We will; toxmy?? Puxforet?PPPOre?ZPPomotor;\n",
      "When; Ifyet?ppoper;; you met;awnexs;out; tomepolj?;\n",
      "Pito;;Plex;\n",
      "Pomes; with;y these?ble?? Putototol???PPPEcoton:\n",
      "Welf,?WVPixxost; their jajtijjes;; at;\n",
      "Fixstoob; that Paving; put;oopjenot? they? are thou?\n",
      "It prothent; by Portazins;;WWhen, commens; though toset my;\n",
      "GoodZPProwes; Porbozed; Pfotly; Pome,; 'toxgetyous;\n",
      "When; afjexcence;jjocconpinged;;;Iny compinged;\n",
      "Pitupunotjoted?GGo!!GPoven!\n",
      "Porved; you; jocesjeswyy whech;owere;;\n",
      "Popnens; Ponzobly; Pirtuz;;Ppot,'s to;;ato;;\n",
      "PondPy; would; I powenwyyy villewwolud's;\n",
      "Woujd Pirtes?mame?GaPery?PPPoptoo!\n",
      "Proz; Pormobly; would; Phocouppary, withootj???\n",
      "Patiznot! Percousen; to;? Purly?PPpotout; they; Prizment?; but Portaply;\n",
      "Well; come? of the; jockffortings would? Pury?OPPProze:\n",
      "Perpor;; theme;;; potianjjjyy? Phicest; Poncoon's;\n",
      "Pantor; Montom the; joke?? But? Porut! Pomower;; come? not there;;\n",
      "Wombly; alj; to; they? will?jjxextor up;ont? Pooro??\n",
      "paptyol'd towertell; Whither; Ponoteplut; witpppopmed;\n",
      "Would I jokesoup to;??WPacofusejt?; Parjion;;\n",
      "Pence, my; Poncaten jetice;;;PPoxsobet;\n",
      "Phat's; axgot; they; must; beyong at woupp??\n",
      "PPEchopout\n",
      "What pavinetep;;himp??? putloy??PPPucouse!!\n",
      "PPelong;\n",
      "Well;; When; Phanish? witljjjy??\n",
      "PPERIP:\n",
      "More than? Peryopupoteloup? of;jment;?-pouchip!ooffxptezto;\n",
      "Mocjepty would bexkenfeble;; when; they; Porest?pptoo;;PProp;oPem;;yy mowent; put; I will; whose?mobj;;Gpet;\n",
      "For; at to cembaly; withoutxxxexfote;;; these; juzstet two??'PPout;PPorto;? pery;;WVence;jotettbus haviz;;GGo;\n",
      "Pentexgenty uponedsed; Mubbet; with; they;\n",
      "Poreswenf; they; Phouldjtersjjucj; Ptonous;WWo; Poneous; Pupitus!\n",
      "Wowput;out upant?Phowe??-Watjjjot??\n",
      "Patost!PPorve??PPaploz?\n",
      "Petol; poon; good; Morke;; wown;;Ppex;; of jeting; up; whose; the;? Potly?PPPopomy;\n",
      "Ppore;;cp;; then; with jiven;?love? of at goxpones;\n",
      "With;jpopedow wat; at Poxpeot;;owet;???\n",
      "\n",
      "KExtGGot;\n",
      "Mizen, Polcomp?? Pony;oWhy? come?? Pet'p;;\n",
      "Fom; Ptinvous;;WPhy; popepor;; Mitne;; Potince; Popeno; Pome;;\n",
      "Gom; Pobes;; when;;out; thousbast; of jeveny; thoughts compent'dybuce?\n",
      "Pot; Phatios! put;IPexcofesjjjj????PPPutoot!MMure! to popent; jexonj?? Poulous; Portuper; Wath;\n",
      "Wouppy;xoffempterourjjucjejsest toxetrize villy?WPPPoxsop:\n",
      "With jovinges;;WPere; you; wewpike; thei;; Pozt;;\n",
      "Mecore wenver;; thinj;;;; my; for;Pomest;; to; pexpy;; wotncegbex;;IPgens, Puccouppous; Poxsoly; would;\n",
      "Fexplyxome;; my; but; as;Iwexly; come; but tho;;WWhy?\n",
      "PPozotor!\n",
      "Pure;\n",
      "Por;o's; some?bebtus;; his bajjezs; then; Poungap; thou wexpinted bleant;\n",
      "Pothary alpxtyechowerbatjard? utot? put;optoop??opprayes?\n",
      "Porly; Purcomen;; Poing;; at;swet them; Potony; Pooptep? wroegs;\n",
      "Wound; these fazesbajj; the, thought; Purponempery?\n",
      "PPREVIMUS:\n",
      "Were you?? Prizeved; flome; both; Post;;PPoubbeven; purpose; I between; wexting;;\n",
      "Mose; for you ware bexore; af they; would worst goody?\n",
      "WWelWozege;\n",
      "We; Pouldes; now; they; are; thou?bIbyx\n",
      "toxpute? of Perow? Palit? Purous; Pitco?;\n",
      "Pozs!oon! truz; themefles;; and then;Pow; at;;\n",
      "Mexbofe; to; putsexle; boing with thoune??;\n",
      "PPozot:\n",
      "Perfy, Phiccousj??\n",
      "PPilopon:\n",
      "Meret??VVoptace???\n",
      "PPortoM:\n",
      "My lord; I will; these?myant; betton; Iwhup; offengs;\n",
      "Mose putpinex'm; they? alettroug??\n",
      "\n",
      "KINGZPHERPPPIPemyoux?jIcepestapt;;oppjepjy; they; 'peaz;\n",
      "Moper you; go;; Petcom;; poes;; Pince; Pomon;; Potpory; Pupant; peyces;; of these forewepp; those; you;?\n",
      "GPeptelome;\n",
      "Pour; go;; pour; pot, vorpexabentyjjojej;;jetce to;net thow; with; the;;\n",
      "Por;;PPent;; Poncapery; Pitconst's venweabun;;jFitj!GPovovous;\n",
      "With compingint?jjcetcost?-PPexloze;\n",
      "Poryous; very burit; it; Pouncomes; that? pound of;\n",
      "Pixteptujionjjoces, with? thy;?outf??PPPoroz!\n",
      "Pore; Porm'n;;?mot;; Fomsoul; to would? Muptot thus;?PPPPicostor;\n",
      "Pet;;apfy; Potony? Which???? Pixto??PPPootom;\n",
      "Por; Ponvese; woust vile?owwelc;; whome;\n",
      "of these?jjjent? good together;;;PPon;; Poncow;;lty; to;ppy;;PPlexebbur; when they joegles foreuty?\n",
      "How!GGou;ZPorte; pow;; put; of Pone?? why? witl;;?\n",
      "Petcomen?\n",
      "Poven! Porvome upot;jtolk? Powet;;-PopkengMofetent; with jopenford;\n",
      "Ficl; joves; Ponteshoput; wouppopt?joup??\n",
      "Fumentol's tazes? at my;jeptisoxswoy;;;PPoves you; will; you;? Puthy? Ppopizen;; Peing; and your pact;\n",
      "Movexjaptwowcworvife;;Witfjeven; as being Jone?\n",
      "Whevelousjjefter;; thenefaly; outjtuppexome;;;WWet;\n",
      "Would; us would sheal?? poundebboubicto;;\n",
      "Fom; and the jamine?? of therefore? Voulcites? not this?\n",
      "Poutlof; Potace; pows! they; will;?pty??\n",
      "PPuchezs:\n",
      "Meaven, I would? not then; Powpor; to;ppeach;\n",
      "Powrated; as they bake; thee; and jeavey; porests;\n",
      "Pownet; but;othel; Poundes; Porteny Poon;;PPors, tome; they;;\n",
      "Pitluse;; they; iflet; Pooz; Povougity;jetous??\n",
      "PPictor;\n",
      "Merazy to; the;?j?? Pixter?PPProzes?\n",
      "Poy; Porce; Ponnow; Pentos; amy the;; Pully;\n",
      "Potosy queptipe;;;ant; Pothoperow;-Woucheggo fouls; your prepace? would; you?? Pray? Pome??PPPoroto:\n",
      "Pere; to; my; and;;btyy atlebe;;ouj; you; anj monicfe;;WPo;;PPitles; Puxiz!! Movery thought? Putlout?pWou;;PPexpefopseod; weej jestow; the;;; my;\n",
      "Ppexy;;jjegswyy; whechengs; to; bean; purp;;one, being; you; woubk??\n",
      "PPexoture;\n",
      "Hes, pexpers;;jamy;jboved to-porve;\n",
      "When;jjy; then; Pownozergal; but for; Pulben; Wexto;\n",
      "Whome to;jprevost; whice; botnfes;; for; to;y them;;bep;;'GGwonge; would I peaffevy; which with pexcold;\n",
      "With bevove whithed with jocemoble; with; these??\n",
      "\n",
      "PPottor:\n",
      "Hex? af them?? our jubbebty?? Pooly?\n",
      "Pour;PPerce; WowWith;; these?s anjstonot? but at the??\n",
      "PPorzose!\n",
      "PBextom; Iwpeore; such; put;ofor; tome;;Pup;;;etibes; and they jovestanjest? Puliuspejosert?\n",
      "Wheplex; would jexuty two warsion pexson? bountos them??WWWecloy? Poilenst;;PPorg;;bfev;; Vobbes; witth;;;PPot;;\n",
      "Pen;; you; betome; put;;oppjepove; atjjwwoxjxwexle? but;\n",
      "When; Poonsajesty; Poztazijesjjycetcounmone;;\n",
      "Pow, Porvemy; Powins; welt; whick; be; wonse;\n",
      "My;GGuse; to; put;ople;; which Panicherpjytweccestodijcesture,\n",
      "Purcugenote;; Pose; Foretty poor; with; they;;Wal;; wetped;jjopjjute; that as thouklong? me;;\n",
      "GowGPare, besome that thou;jhervent poor; but not?PPPaxiz!! Put, Purpize?? We?? Pould??PPProvoso:\n",
      "Perom? there's?jjatt?? Putiz??PPPovoze:\n",
      "Porco;; my; peaco;; peen;; pouns; with;out; power;\n",
      "Wowj; queet; allotuge joweb?;;;\n",
      "Fitry; Potizunwy; which you? peace;\n",
      "Mose toone, goove? Pirjovojy power;; juince so;ath; Potcoskem;;\n",
      "For; tell; be; Pome;; towby; you;; potejre?;\n",
      "Pecpevent; of Mostame subjection;; Pownot thou, twany??PPPPiPoRot!\n",
      "When, Vorjazs?? Poroto;;Pant; themenome, Poxzozo;;\n",
      "Five; by; ot Pusiz; his would; they know? Pelpory;\n",
      "Would them of;jto;;WWeccoupeto't out??\n",
      "Preazou\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3e285a41f56befe9dfff401b1359138cb6732e15a350f6ffd09efa79c669fb1c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('LSTM_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
