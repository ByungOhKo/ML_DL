{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = model_dim // num_heads\n",
    "        \n",
    "        assert (self.head_dim * num_heads == model_dim), \"Embed size needs to be div by heads!\"\n",
    "        \n",
    "        # Scaled Dot-Product Attention은 Head로 나누어진다.\n",
    "        # (N, L, model_dim) -> (N, L, num_heads, head_dim)\n",
    "        self.embed2Q = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.embed2K = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.embed2V = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(self.model_dim, model_dim)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask):\n",
    "        # Q, K, V: (N, L, d_model)\n",
    "        N = Q.shape[0]\n",
    "        L = Q.shape[1]\n",
    "        # 1. Multi-Head Split\n",
    "        Q = Q.reshape((N, L, self.num_heads, self.head_dim))\n",
    "        K = K.reshape((N, L, self.num_heads, self.head_dim))\n",
    "        V = V.reshape((N, L, self.num_heads, self.head_dim))\n",
    "        \n",
    "        Q = self.embed2Q(Q)\n",
    "        K = self.embed2K(K)\n",
    "        V = self.embed2V(V)\n",
    "        # 2. MatMul(Q, K)\n",
    "        # (N, Q_L, h, d_h) * (N, K_L, h, d_h) -> (N, h, Q_L, K_L)\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [Q, K])\n",
    "        # 3. Scale\n",
    "        energy /= self.head_dim ** (1/2)\n",
    "        # 4. Mask (opt.)\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        # 5. Softmax\n",
    "        attention = torch.softmax(energy, dim=3)\n",
    "        # 6. MatMul(attention, V), Concat\n",
    "        # (N, h, Q_L, K_L) * (N, V_L, h, d_h) -> (N, query_L, h, d_h)\n",
    "        # (N, query_L, h, d_h) -> (N, L, d_model)\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, V]).reshape((N, L, self.model_dim))\n",
    "        # 8. Linear\n",
    "        out = self.fc_out(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNSubLayer(nn.Module):\n",
    "    def __init__(self, model_dim, ff_dim, dropout):\n",
    "        super(FFNSubLayer, self).__init__()\n",
    "        self.norm = nn.LayerNorm(model_dim)\n",
    "        self.fc1 = nn.Linear(model_dim, ff_dim)\n",
    "        self.fc2 = nn.Linear(ff_dim, model_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        sublayer_x = self.dropout(self.fc2(self.relu(self.fc1(x))))\n",
    "        out = self.norm(x + sublayer_x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttentionSubLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads):\n",
    "        super(MultiHeadAttentionSubLayer, self).__init__()\n",
    "        self.norm = nn.LayerNorm(model_dim)\n",
    "        self.attention = SelfAttention(model_dim, num_heads)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask):\n",
    "        attention = self.attention(Q, K, V, mask)\n",
    "        \n",
    "\n",
    "class Encoder(nn.Module):\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c7db64f36cbcc5fb7ccbfb37c66e98c907907712b48be2e35e1ce3a3f867289"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
